===============================================================================
                            ğŸ“‹ TL;DR - QUICK START
===============================================================================

ğŸš€ ONE-COMMAND DEPLOYMENT:
   docker compose up -d

ğŸ¤– CI/CD PIPELINE:
   â€¢ GitHub Actions: Automated ML pipeline (data â†’ features â†’ training)
   â€¢ Triggers: Push to main/develop, PR, or manual workflow dispatch
   â€¢ Artifacts: Processed data, preprocessor, trained model, MLflow logs

ğŸ“ ACCESS POINTS:
   â€¢ Streamlit UI:  http://localhost:8501  (Web Interface)
   â€¢ FastAPI:       http://localhost:8000  (REST API)
   â€¢ MLflow:        http://localhost:5555  (Experiment Tracking)
   â€¢ GitHub Actions: Repository â†’ Actions tab

âœ… VERIFICATION:
   curl http://localhost:8000/health
   # Expected: {"status":"healthy","model_loaded":true}

ğŸ“¦ WHAT'S INCLUDED:
   â€¢ FastAPI service (ML model predictions API)
   â€¢ Streamlit web UI (user-friendly interface)
   â€¢ Docker Compose orchestration with health checks
   â€¢ GitHub Actions ML pipeline (CI/CD automation)
   â€¢ Automatic service discovery and networking
   â€¢ scikit-learn 1.6.1 for model compatibility

ğŸ“‚ KEY FILES:
   â€¢ Dockerfile (FastAPI service)
   â€¢ streamlit_app/Dockerfile (Streamlit UI)
   â€¢ docker-compose.yaml (orchestration)
   â€¢ .github/workflows/ml-pipeline.yml (CI/CD pipeline)
   â€¢ .claude/commands/update-changelog.md (custom slash command)

ğŸ›‘ STOP SERVICES:
   docker compose down

ğŸ“– For detailed documentation, see sections below â¬‡ï¸

===============================================================================

===============================================================================
  ğŸš€ GITHUB ACTIONS ML PIPELINE ENHANCEMENTS - 2025-11-21
===============================================================================

Summary:
Enhanced GitHub Actions ML pipeline with MLflow Docker integration, Docker image
build/publish workflow, version tag triggers, and improved artifact management.

What Was Done:

1. Added MLflow Docker Setup in Model Training Job:
   - Docker-based MLflow server (ghcr.io/mlflow/mlflow:latest)
   - Runs on port 5000 with SQLite backend
   - Health check with retry logic (10 attempts, 5s intervals)
   - Automatic cleanup using if: always() condition
   - Updated tracking URI from file:./mlruns to http://localhost:5000

2. Added Build and Publish Docker Image Job:
   - Conditional execution: runs only on main branch or version tags
   - Environment: dev (uses DockerHub credentials)
   - Downloads trained model and preprocessor artifacts
   - Artifact verification step before build
   - Docker Buildx setup for multi-platform builds
   - DockerHub authentication with secrets
   - Smart tag generation using docker/metadata-action:
     * latest (for main branch)
     * Semantic versioning (v1.0.0, 1.0)
     * SHA-based tags (main-sha256:abc123)
   - Build cache optimization for faster builds

3. Added Version Tag Triggers:
   - Pipeline now triggers on git tags matching v*.*.*
   - Enables automated deployment on version releases
   - Combined with existing push and PR triggers

4. Added Preprocessor Download in Model Training:
   - Downloads preprocessor artifact from feature-engineering job
   - Ensures model training has access to fitted preprocessor
   - Path: models/trained/preprocessor.pkl

5. Updated Pipeline Summary Job:
   - Now includes build-and-publish status
   - Smart handling of conditional jobs (shows "skipped" when not running)
   - Enhanced artifact list in summary report
   - Better status reporting for all 5 jobs

-------------------------------------------------------------------------------
COMMANDS/INSTRUCTIONS:
-------------------------------------------------------------------------------

# Trigger pipeline with version tag (triggers build-and-publish):
git tag v1.0.0
git push origin v1.0.0

# Push to main (triggers full pipeline with Docker build):
git push origin main

# Manual workflow trigger:
gh workflow run ml-pipeline.yml

# View Docker images in registry:
# Visit: https://hub.docker.com/r/<username>/house-price-model

-------------------------------------------------------------------------------
MLFLOW DOCKER SETUP:
-------------------------------------------------------------------------------

# MLflow server starts automatically in GitHub Actions:
docker pull ghcr.io/mlflow/mlflow:latest
docker run -d -p 5000:5000 --name mlflow-server \
  ghcr.io/mlflow/mlflow:latest \
  mlflow server --host 0.0.0.0 --backend-store-uri sqlite:///mlflow.db

# Health check (automatically retried):
curl -f http://localhost:5000/health

# Cleanup (automatic in pipeline):
docker stop mlflow-server || true
docker rm mlflow-server || true

-------------------------------------------------------------------------------
DOCKER IMAGE TAGGING STRATEGY:
-------------------------------------------------------------------------------

Tag Types Generated:
â”œâ”€ latest                    (main branch latest)
â”œâ”€ v1.0.0, 1.0              (semantic version tags)
â””â”€ main-sha256:abc123       (commit SHA for traceability)

Example: Pushing tag v1.2.3 creates:
- docker.io/<username>/house-price-model:latest
- docker.io/<username>/house-price-model:v1.2.3
- docker.io/<username>/house-price-model:1.2
- docker.io/<username>/house-price-model:main-sha256:...

-------------------------------------------------------------------------------
DEPLOYMENT STATUS/RESULTS:
-------------------------------------------------------------------------------

âœ… MLflow Docker Integration: Configured with health checks
âœ… Build and Publish Job: Created with DockerHub integration
âœ… Version Tag Triggers: Added v*.*.* pattern
âœ… Preprocessor Download: Added to model-training job
âœ… Pipeline Summary: Enhanced with build status
âœ… Environment Configuration: dev environment with DockerHub secrets

Pipeline Architecture (Updated):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Processing    â”‚  â† Cleans raw CSV
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature Engineering â”‚  â† Creates features & preprocessor
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Training     â”‚  â† Trains with MLflow Docker
â”‚  + MLflow Server    â”‚     (NEW: Docker-based tracking)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Build & Publish     â”‚  â† Builds & pushes Docker image
â”‚  (conditional)      â”‚     (NEW: Only on main/tags)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pipeline Summary    â”‚  â† Enhanced status report
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

-------------------------------------------------------------------------------
Notes:
- Build-and-publish job only runs on main branch pushes or version tags
- MLflow server uses ephemeral SQLite DB (resets each run)
- Docker build cache improves build performance
- Requires DOCKERHUB_USERNAME (var) and DOCKERHUB_TOKEN (secret) in dev environment
- Preprocessor is now properly passed between feature-engineering and model-training
- All jobs use latest GitHub Actions (v4, v5)

===============================================================================

===============================================================================
  GITHUB ACTIONS ML PIPELINE - 2025-11-21
===============================================================================

Summary:
Successfully created GitHub Actions workflow for automated ML pipeline from
data processing through model training with comprehensive CI/CD automation.

What Was Done:

1. Created GitHub Actions Workflow (.github/workflows/ml-pipeline.yml):
   - 4-job pipeline: Data Processing â†’ Feature Engineering â†’ Model Training â†’ Summary
   - Automated triggers: Push to main/develop, PRs, manual dispatch
   - Artifact management with retention policies (7-30 days)
   - Python 3.13 with dependency caching
   - Health checks and verification at each step
   - Comprehensive error handling and reporting

2. Pipeline Jobs Configuration:

   Job 1 - Data Processing:
   - Runs src/data/run_processing.py
   - Outputs: cleaned_house_data.csv
   - Artifact: processed-data (7 days retention)

   Job 2 - Feature Engineering:
   - Runs src/features/engineer.py
   - Depends on: data-processing
   - Outputs: featured_house_data.csv, preprocessor.pkl
   - Artifacts: featured-data, preprocessor (7 days retention)

   Job 3 - Model Training:
   - Runs src/models/train_model.py
   - Depends on: feature-engineering
   - Outputs: house_price_model.pkl, MLflow artifacts
   - Artifacts: trained-model (30 days), mlflow-artifacts (30 days)
   - Optional: Can be disabled via workflow dispatch

   Job 4 - Pipeline Summary:
   - Aggregates status from all jobs
   - Creates GitHub Actions summary report
   - Fails if any upstream job fails

3. Updated Source Scripts:
   - Modified src/data/run_processing.py to support CLI arguments
   - Added argparse with --input and --output parameters
   - Maintains backward compatibility

4. Created Comprehensive Documentation:
   - .github/workflows/README.md with full pipeline documentation
   - Architecture diagrams and workflow visualization
   - Manual execution instructions (UI and CLI)
   - Artifact management guide
   - Troubleshooting section
   - Local testing instructions

5. Created Custom Slash Command:
   - .claude/commands/update-changelog.md
   - Interactive changelog update command
   - Maintains consistent formatting
   - Usage: /update-changelog

-------------------------------------------------------------------------------
GITHUB ACTIONS WORKFLOW COMMANDS:
-------------------------------------------------------------------------------

# Trigger workflow manually (via GitHub CLI):
gh workflow run ml-pipeline.yml

# Run without model training:
gh workflow run ml-pipeline.yml -f run_training=false

# Run on specific branch:
gh workflow run ml-pipeline.yml --ref develop

# List recent workflow runs:
gh run list --workflow=ml-pipeline.yml

# View workflow run details:
gh run view <run-id>

# Watch running workflow:
gh run watch <run-id>

# Download all artifacts:
gh run download <run-id>

# Download specific artifact:
gh run download <run-id> -n trained-model

-------------------------------------------------------------------------------
PIPELINE ARCHITECTURE:
-------------------------------------------------------------------------------

Workflow Triggers:
â”œâ”€ Push to main/develop (when src/, data/raw/, configs/ change)
â”œâ”€ Pull Requests to main/develop
â””â”€ Manual workflow_dispatch (with optional parameters)

Pipeline Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Processing    â”‚  â† Cleans raw CSV data
â”‚  (ubuntu-latest)    â”‚     Artifact: processed-data.csv
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ depends_on
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature Engineering â”‚  â† Creates features & preprocessor
â”‚  (ubuntu-latest)    â”‚     Artifacts: featured-data.csv, preprocessor.pkl
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ depends_on
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model Training     â”‚  â† Trains model with MLflow
â”‚  (ubuntu-latest)    â”‚     Artifacts: model.pkl, mlflow logs
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ always()
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pipeline Summary    â”‚  â† Status report & validation
â”‚  (ubuntu-latest)    â”‚     GitHub Actions Summary
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

-------------------------------------------------------------------------------
ARTIFACTS & RETENTION:
-------------------------------------------------------------------------------

| Artifact          | Retention | Size (Approx) | Description                |
|-------------------|-----------|---------------|----------------------------|
| processed-data    | 7 days    | ~50 KB        | Cleaned CSV data           |
| featured-data     | 7 days    | ~75 KB        | Engineered features CSV    |
| preprocessor      | 7 days    | ~10 KB        | Sklearn preprocessor.pkl   |
| trained-model     | 30 days   | ~500 KB       | Trained model.pkl          |
| mlflow-artifacts  | 30 days   | ~1 MB         | MLflow experiment tracking |

-------------------------------------------------------------------------------
DEPLOYMENT STATUS:
-------------------------------------------------------------------------------

âœ… GitHub Actions Workflow: Created and ready
âœ… Pipeline Jobs: 4 jobs configured with dependencies
âœ… Artifact Management: Automated upload/download between jobs
âœ… Documentation: Comprehensive README in .github/workflows/
âœ… CLI Support: All scripts support command-line arguments
âœ… Custom Commands: /update-changelog slash command available

Pipeline Features:
- Automatic triggers on code/data changes
- Manual dispatch with parameters
- Python dependency caching for faster runs
- Comprehensive error handling and verification
- GitHub Actions summary reports
- MLflow integration for experiment tracking

-------------------------------------------------------------------------------
TESTING LOCALLY (Before Pushing):
-------------------------------------------------------------------------------

# Test data processing:
python src/data/run_processing.py \
  --input data/raw/house_data.csv \
  --output data/processed/cleaned_house_data.csv

# Test feature engineering:
python src/features/engineer.py \
  --input data/processed/cleaned_house_data.csv \
  --output data/processed/featured_house_data.csv \
  --preprocessor models/trained/preprocessor.pkl

# Test model training:
python src/models/train_model.py \
  --config configs/model_config.yaml \
  --data data/processed/featured_house_data.csv \
  --models-dir models

-------------------------------------------------------------------------------
Notes:
- Pipeline runs automatically on push to main/develop branches
- Artifacts are stored in GitHub Actions for specified retention periods
- Model training can be skipped using workflow dispatch parameter
- All jobs run on Ubuntu latest with Python 3.13
- Dependency caching speeds up subsequent runs
- MLflow tracking uses local file storage (file:./mlruns)
- Custom slash command /update-changelog simplifies changelog updates

===============================================================================

===============================================================================

===============================================================================
  DOCKER COMPOSE ORCHESTRATION - 2025-11-20
===============================================================================

Summary:
Successfully created and deployed docker-compose.yaml to orchestrate both FastAPI
and Streamlit services with proper networking and health checks.

What Was Done:

1. Created docker-compose.yaml file in project root:
   - Orchestrates FastAPI (port 8000) and Streamlit (port 8501) services
   - Custom bridge network: house-price-network
   - Automatic service discovery (Streamlit connects to FastAPI via service name)
   - Health checks for FastAPI service with Python-based HTTP check
   - Dependency management: Streamlit waits for FastAPI to be healthy
   - Auto-restart policy: unless-stopped

2. Service Configuration:
   FastAPI Service:
   - Container name: house-price-api
   - Health check using Python urllib (no curl dependency)
   - 20s start period, 30s interval, 3 retries

   Streamlit Service:
   - Container name: house-price-streamlit
   - Environment: API_URL=http://fastapi:8000
   - Depends on FastAPI health check

3. Deployed and Verified:
   - Both services running and healthy
   - Network communication working correctly
   - API predictions working via docker-compose networking
   - Streamlit can access FastAPI via service name

-------------------------------------------------------------------------------
DOCKER COMPOSE COMMANDS:
-------------------------------------------------------------------------------

# Start all services (build if needed):
docker compose up -d

# Start with rebuild:
docker compose up -d --build

# View running services:
docker compose ps

# View logs (all services):
docker compose logs -f

# View logs (specific service):
docker compose logs -f fastapi
docker compose logs -f streamlit

# Stop all services:
docker compose down

# Stop and remove volumes:
docker compose down -v

# Restart services:
docker compose restart

# Check service health:
docker compose ps

-------------------------------------------------------------------------------
TESTING THE DEPLOYMENT:
-------------------------------------------------------------------------------

# Test FastAPI health endpoint:
curl http://localhost:8000/health

# Test FastAPI prediction:
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "sqft": 2000,
    "bedrooms": 4,
    "bathrooms": 3,
    "location": "urban",
    "year_built": 2015,
    "condition": "excellent"
  }'

# Expected Response:
# {"predicted_price":566340.0,"confidence_interval":[509706.0,622974.0],...}

# Access Streamlit UI:
# Open browser: http://localhost:8501

-------------------------------------------------------------------------------
DEPLOYMENT STATUS:
-------------------------------------------------------------------------------

âœ… FastAPI Service: http://localhost:8000 (healthy)
âœ… Streamlit UI: http://localhost:8501 (running)
âœ… Docker Network: house-price-network (bridge)

Service Details:
- fastapi: house-price-api container on port 8000
- streamlit: house-price-streamlit container on port 8501
- Network: Services communicate via Docker DNS (fastapi hostname)

-------------------------------------------------------------------------------
ARCHITECTURE:
-------------------------------------------------------------------------------

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Docker Compose Stack                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚   Streamlit UI   â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚   FastAPI API    â”‚         â”‚
â”‚  â”‚   Port: 8501     â”‚         â”‚   Port: 8000     â”‚         â”‚
â”‚  â”‚                  â”‚         â”‚                  â”‚         â”‚
â”‚  â”‚  house-price-    â”‚  HTTP   â”‚  house-price-    â”‚         â”‚
â”‚  â”‚  streamlit       â”‚         â”‚  api             â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                              â”‚                     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                        â”‚                                     â”‚
â”‚              house-price-network                             â”‚
â”‚                   (bridge)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â”‚                        â”‚
    localhost:8501          localhost:8000

-------------------------------------------------------------------------------
Notes:
- Services communicate via Docker network using service names (fastapi, streamlit)
- Health checks ensure FastAPI is ready before Streamlit starts
- No need for host.docker.internal or IP addresses with docker-compose
- Services restart automatically unless manually stopped
- Logs are aggregated and accessible via docker compose logs

===============================================================================

===============================================================================
  STREAMLIT APP DOCKER DEPLOYMENT - 2025-11-20
===============================================================================

Summary:
Successfully created, built, and deployed the Streamlit web application Docker container.

What Was Done:

1. Created Dockerfile (streamlit_app/Dockerfile) following the specifications from README:
   - Base image: python:3.9-slim
   - Working directory: /app
   - Copied all files from streamlit_app directory (app.py, requirements.txt, .streamlit config)
   - Installed dependencies from requirements.txt
   - Exposed port 8501
   - Launch command: streamlit run app.py --server.address=0.0.0.0

2. Built Docker Image:
   - Successfully built image: house-price-streamlit
   - All dependencies installed (streamlit, requests, pyarrow, altair, pandas, etc.)
   - Image optimized with --no-cache-dir flag

3. Deployed and Verified:
   - Container running on port 8501
   - Connected to FastAPI service via API_URL=http://host.docker.internal:8000
   - Streamlit app is accessible and operational

-------------------------------------------------------------------------------
STREAMLIT APP BUILD & RUN COMMANDS:
-------------------------------------------------------------------------------

# Build the Streamlit image:
cd streamlit_app
docker build -t house-price-streamlit .

# Run the container (standalone):
docker run -p 8501:8501 house-price-streamlit

# Run with FastAPI connection (recommended - use host.docker.internal for Mac/Windows):
docker run -p 8501:8501 -e API_URL=http://host.docker.internal:8000 house-price-streamlit

# For Linux, use host network or the host IP:
docker run -p 8501:8501 -e API_URL=http://172.17.0.1:8000 house-price-streamlit

# Access the Streamlit app:
# Open browser: http://localhost:8501

# Verify both containers are running:
docker ps

-------------------------------------------------------------------------------
DEPLOYMENT STATUS:
-------------------------------------------------------------------------------

âœ… FastAPI Service: Running on http://localhost:8000
âœ… Streamlit UI: Running on http://localhost:8501
âœ… MLflow Server: Running on http://localhost:5555

Container Details:
- house-price-api: Port 8000 (predictions API)
- house-price-streamlit: Port 8501 (web interface)

-------------------------------------------------------------------------------
Notes:
- The Streamlit app connects to the FastAPI service via the API_URL environment variable
- Default API_URL is http://model:8000 (for docker-compose networking)
- For standalone containers on Mac/Windows, use host.docker.internal instead of localhost
- For Linux, use the Docker bridge IP (typically 172.17.0.1) or host network mode

===============================================================================

===============================================================================
  FASTAPI DOCKER DEPLOYMENT - 2025-11-20
===============================================================================

Summary:
Successfully created and deployed the FastAPI Docker container for house price prediction.

What Was Done:

1. Created Dockerfile (/Dockerfile) following the specifications from the API README:
   - Base image: python:3.11-slim
   - Working directory: /app
   - Installed dependencies from requirements.txt
   - Copied API source files and trained models
   - Exposed port 8000
   - Launch command: uvicorn main:app --host 0.0.0.0 --port 8000

2. Fixed scikit-learn Version Compatibility:
   - Updated requirements.txt to pin scikit-learn==1.6.1 to match the trained models
   - Rebuilt the Docker image with the correct version

3. Successfully Deployed:
   - Container is running on port 8000
   - Health check endpoint working: {"status":"healthy","model_loaded":true}
   - Prediction endpoint working successfully with test data

-------------------------------------------------------------------------------
API VERIFICATION COMMANDS:
-------------------------------------------------------------------------------

# Build the Docker image:
docker build -t house-price-api .

# Run the container:
docker run -p 8000:8000 house-price-api

# Test Health Check endpoint:
curl http://localhost:8000/health

# Expected Response:
# {"status":"healthy","model_loaded":true}

# Test Prediction endpoint:
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "sqft": 1500,
    "bedrooms": 3,
    "bathrooms": 2,
    "location": "suburban",
    "year_built": 2000,
    "condition": "fair"
  }'

# Expected Response:
# {"predicted_price":429840.0,"confidence_interval":[386856.0,472824.0],"features_importance":{},"prediction_time":"2025-11-20T10:47:33.665754"}

-------------------------------------------------------------------------------
API Endpoints Available:
-------------------------------------------------------------------------------

- Health Check: GET http://localhost:8000/health
- Prediction: POST http://localhost:8000/predict
- Batch Prediction: POST http://localhost:8000/batch-predict

The Docker container is now running and ready to serve house price predictions!

===============================================================================
  
Created the Dockerfile at the root of your project. Here's what it does:

  Key Features:
  - Base Image: python:3.11-slim as specified
  - Working Directory: /app
  - Dependencies: Installs from requirements.txt
  - API Files: Copies main.py, schemas.py, and inference.py from src/api/
  - Models: Copies both trained model files from models/trained/
  - Port: Exposes port 8000
  - Launch: Uses uvicorn main:app --host 0.0.0.0 --port 8000

  The container structure matches the expected layout from the README with all
  files under /app and models in the models/trained/ subdirectory.

  You can build and run it with:
  docker build -t house-price-api .
  docker run -p 8000:8000 house-price-api

Excellent! The model training completed successfully! ğŸ‰

  Training Results:

  âœ… Model: RandomForestâœ… Performance Metrics:
  - MAE (Mean Absolute Error): $14,381.25
  - RÂ² Score: 0.9825 (98.25% - excellent fit!)

  MLflow Tracking:
  - Experiment created: house_price_model
  - Run logged at:
  http://localhost:5555/#/experiments/2/runs/3df5e95e1d5b411cba8b2b2f2347888c
  - Model registered in MLflow Model Registry as version 1
  - Model transitioned to "Staging" stage

  Artifacts Saved:
  - Trained model: models/trained/house_price_model.pkl
  - MLflow experiment tracking at http://localhost:5555

  Note: There's a deprecation warning about model registry stages - this is
  just a heads-up that MLflow will change this feature in future versions, but
  it works fine for now.

  ---
  Next Steps - Deployment Options:

  1. Test the FastAPI service:
    - Build and run the Docker container
    - Test predictions via REST API
  2. Launch Streamlit UI:
    - Deploy the web interface
    - Test predictions through the browser
  3. View MLflow UI:
    - Visit http://localhost:5555 to see experiment tracking and registered
  models

  The feature engineering script executed successfully. Here's what
  was accomplished:

  âœ… Feature Engineering Complete

  Features Created:
  - house_age: Calculated from year_built
  - price_per_sqft: Price normalized by square footage
  - bed_bath_ratio: Bedrooms to bathrooms ratio

  Processing Summary:
  - Input: 77 rows from cleaned data
  - Output: 77 rows with 10 features (original + engineered)
  - Preprocessor fitted and saved to models/trained/preprocessor.pkl
  - Transformed data saved to data/processed/featured_house_data.csv

  Next Steps:
  You can now proceed with model training:

  python3 src/models/train_model.py \
    --config configs/model_config.yaml \
    --data data/processed/featured_house_data.csv \
    --models-dir models \
    --mlflow-tracking-uri http://localhost:5555