================================================================================
MLOPS BEST PRACTICES AND PATTERNS - UNIVERSAL GUIDE
================================================================================

This document provides universal MLOps best practices, patterns, and guidelines
applicable to any machine learning project, regardless of tech stack or domain.
Based on real-world experience and industry standards.

================================================================================
TABLE OF CONTENTS
================================================================================

1. MLOps Pipeline Architecture Patterns
2. Experiment Tracking and Model Registry
3. Feature Engineering Best Practices
4. Model Training and Evaluation
5. Model Deployment Strategies
6. CI/CD for ML Projects
7. Common Pitfalls and How to Avoid Them
8. Debugging and Troubleshooting
9. Production Readiness Checklist

================================================================================
1. MLOPS PIPELINE ARCHITECTURE PATTERNS
================================================================================

CORE PRINCIPLE: Separation of Concerns
--------------------------------------
Always separate your ML pipeline into distinct, modular stages:

Stage 1: Data Processing
- Input: Raw data from various sources
- Output: Cleaned, validated data
- Responsibilities:
  * Data validation and quality checks
  * Missing value handling
  * Outlier detection and removal
  * Data type conversions
  * Duplicate removal

Stage 2: Feature Engineering
- Input: Cleaned data
- Output: Transformed features + fitted preprocessor/transformer
- Responsibilities:
  * Feature creation and derivation
  * Feature scaling/normalization
  * Categorical encoding
  * Feature selection
  * **CRITICAL**: Save fitted transformers for inference

Stage 3: Model Training
- Input: Featured data + model configuration
- Output: Trained model + evaluation metrics
- Responsibilities:
  * Model selection and hyperparameter tuning
  * Train-test splitting
  * Model fitting
  * Evaluation and metrics logging
  * Model serialization

Stage 4: Model Evaluation
- Input: Trained model + validation/test data
- Output: Performance metrics + visualizations
- Responsibilities:
  * Cross-validation
  * Performance metrics calculation
  * Error analysis
  * Model comparison

Stage 5: Model Deployment
- Input: Production-ready model
- Output: Inference service/API
- Responsibilities:
  * Model loading and initialization
  * Request validation
  * Feature transformation (using saved transformers)
  * Prediction generation
  * Response formatting


WHY THIS SEPARATION MATTERS:
---------------------------
✓ Each stage can be tested independently
✓ Easy to debug specific components
✓ Enables parallel development
✓ Simplifies CI/CD pipeline creation
✓ Makes it easy to retrain specific stages
✓ Improves code reusability


ANTI-PATTERN: All-in-one scripts
--------------------------------
❌ DON'T: Create single scripts that do data processing, feature engineering,
         and model training all at once
✓ DO: Create modular scripts with clear inputs and outputs


PATTERN: Command-Line Interface for Each Stage
---------------------------------------------
Each pipeline stage should accept command-line arguments:

✓ DO:
python src/data/process.py --input raw.csv --output cleaned.csv
python src/features/engineer.py --input cleaned.csv --output features.csv --transformer transformer.pkl
python src/models/train.py --config config.yaml --data features.csv --output model.pkl

❌ DON'T: Hard-code file paths inside scripts


PATTERN: Configuration Over Code
--------------------------------
Use configuration files (YAML/JSON) for:
- Model hyperparameters
- Feature engineering parameters
- Data processing rules
- Pipeline settings

✓ DO:
# config.yaml
model:
  type: "RandomForest"
  params:
    n_estimators: 100
    max_depth: 10

❌ DON'T: Hard-code hyperparameters in training scripts

================================================================================
2. EXPERIMENT TRACKING AND MODEL REGISTRY
================================================================================

CRITICAL RULE: Always Track Experiments
---------------------------------------
Never train models without experiment tracking.

WHAT TO LOG:
-----------
1. Hyperparameters (ALL of them, including defaults)
2. Metrics (training, validation, test)
3. Model artifacts
4. Dataset versions/checksums
5. Feature names and transformations
6. System information:
   - Python version
   - Library versions (scikit-learn, tensorflow, etc.)
   - Hardware specs (CPU/GPU)
   - Training duration
7. Git commit hash
8. Random seeds
9. Data splits (train/val/test sizes)


MLFLOW BEST PRACTICES:
---------------------

1. Experiment Naming Convention:
   ✓ DO: Use descriptive names like "house_price_xgboost_v2"
   ❌ DON'T: Use generic names like "experiment_1"

2. Run Naming:
   ✓ DO: Include key info like "lr_0.01_batch_128_epoch_50"
   ❌ DON'T: Use default auto-generated names

3. Experiment Organization:
   - One experiment per model type or use case
   - Use tags to categorize runs
   - Add descriptions to experiments

4. Model Registry Usage:
   - Register only production-worthy models
   - Use stages: None → Staging → Production → Archived
   - Add version descriptions explaining changes
   - Tag models with metadata

5. Artifact Storage:
   - Log preprocessors/transformers alongside models
   - Save feature engineering code snapshots
   - Include model evaluation plots
   - Store example predictions


EXPERIMENT CREATION PATTERN:
---------------------------

✓ DO: Create or get experiment explicitly

import mlflow

# This creates experiment if it doesn't exist
experiment = mlflow.set_experiment("my_experiment")
print(f"Experiment ID: {experiment.experiment_id}")

with mlflow.start_run(run_name="descriptive_run_name"):
    # Log everything
    mlflow.log_params(hyperparameters)
    mlflow.log_metrics(metrics)
    mlflow.log_artifact("model.pkl")


❌ DON'T: Assume experiment exists or use default experiment

with mlflow.start_run():  # Uses default experiment (bad practice)
    pass


CRITICAL BUG TO AVOID:
---------------------
ISSUE: "Could not find experiment with ID 0"

ROOT CAUSE:
- Trying to use an experiment that doesn't exist
- Not creating experiment before starting run
- Conditional experiment creation (only when tracking URI is set)

SOLUTION:
# ALWAYS call set_experiment unconditionally
if tracking_uri:
    mlflow.set_tracking_uri(tracking_uri)

# This line should ALWAYS execute (not inside if block)
experiment = mlflow.set_experiment(experiment_name)


TRACKING BACKEND CHOICES:
------------------------
Development:
- File-based: mlruns/ directory (simple, local)
- Warning: "Filesystem tracking backend deprecated" is expected

Production:
- Database backend: PostgreSQL, MySQL
- Format: "postgresql://user:pass@host:port/db"
- Or cloud: Databricks, AWS SageMaker, Azure ML

================================================================================
3. FEATURE ENGINEERING BEST PRACTICES
================================================================================

GOLDEN RULE: Training-Inference Consistency
-------------------------------------------
Feature engineering MUST be identical in training and inference.

THE PREPROCESSOR PATTERN:
-------------------------

✓ CORRECT APPROACH:

# Training Phase (feature engineering script)
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
import pickle

# Create transformations
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numerical_features),
    ('cat', OneHotEncoder(), categorical_features)
])

# Fit on training data
preprocessor.fit(X_train)

# Save the FITTED preprocessor
with open('preprocessor.pkl', 'wb') as f:
    pickle.dump(preprocessor, f)

# Transform data
X_transformed = preprocessor.transform(X)


# Inference Phase (API/serving)
import pickle

# Load the SAME preprocessor
with open('preprocessor.pkl', 'rb') as f:
    preprocessor = pickle.load(f)

# Apply SAME transformations
X_new_transformed = preprocessor.transform(X_new)


❌ WRONG APPROACH: Re-creating transformations

# Training
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Inference (WRONG - creates NEW scaler)
scaler = StandardScaler()  # This is a DIFFERENT scaler!
X_new_scaled = scaler.fit_transform(X_new)  # WRONG - fitting on new data!


FEATURE ENGINEERING CHECKLIST:
------------------------------
□ All transformations are in a single pipeline/preprocessor
□ Preprocessor is fitted ONLY on training data
□ Fitted preprocessor is saved as artifact
□ Same preprocessor is loaded during inference
□ Feature names are consistent across train/inference
□ Feature order is preserved
□ All feature engineering logic is version controlled


WHEN TO RETRAIN PREPROCESSOR:
-----------------------------
You MUST retrain the preprocessor when:
✓ Adding new features
✓ Removing features
✓ Changing transformation logic
✓ Updating encoding schemes
✓ Modifying scaling parameters
✓ Changing categorical categories

Process:
1. Modify feature engineering code
2. Re-run feature engineering script (creates new preprocessor)
3. Re-run model training (with new preprocessor)
4. Deploy both new model AND new preprocessor together


FEATURE ENGINEERING ANTI-PATTERNS:
----------------------------------

❌ Anti-Pattern 1: Feature leakage
Example: Using test set statistics in transformations
DON'T: scaler.fit(pd.concat([X_train, X_test]))
DO: scaler.fit(X_train)

❌ Anti-Pattern 2: Inconsistent feature engineering
Example: Different logic in training vs inference
DON'T: Have separate feature engineering code for train/serve
DO: Use the same preprocessor artifact

❌ Anti-Pattern 3: Hard-coded feature values
Example: Manual scaling with hard-coded mean/std
DON'T: X_scaled = (X - 50) / 10  # Where do 50 and 10 come from?
DO: Use fitted scaler from training

❌ Anti-Pattern 4: Missing features in inference
Example: Features used in training not available in inference
DO: Document required features in schema
DO: Validate input features before prediction

================================================================================
4. MODEL TRAINING AND EVALUATION
================================================================================

PRE-TRAINING CHECKLIST:
----------------------
□ Data is properly split (train/validation/test)
□ No data leakage between splits
□ Features are properly engineered
□ Baseline model established
□ Evaluation metrics defined
□ Random seeds set for reproducibility


TRAIN-TEST SPLIT BEST PRACTICES:
--------------------------------

✓ DO: Use stratified splitting for classification
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,  # Always set for reproducibility
    stratify=y  # For classification
)

✓ DO: Use time-based splitting for time series
# For time series: chronological split
split_date = '2023-01-01'
train = df[df['date'] < split_date]
test = df[df['date'] >= split_date]

❌ DON'T: Shuffle time series data
❌ DON'T: Use random splits for time-dependent data


HYPERPARAMETER TUNING:
---------------------

Pattern 1: Grid Search
- Exhaustive search over parameter grid
- Use for small parameter spaces
- Example: 2-3 parameters with few values each

Pattern 2: Random Search
- Random sampling from parameter distributions
- More efficient than grid search
- Use for larger parameter spaces

Pattern 3: Bayesian Optimization
- Smart search using previous results
- Most efficient for expensive models
- Tools: Optuna, Hyperopt

✓ DO: Always use cross-validation during tuning
✓ DO: Log ALL hyperparameter combinations tried
❌ DON'T: Tune on test set (use validation set)


MODEL EVALUATION METRICS:
------------------------

Regression:
- MAE (Mean Absolute Error): Easy to interpret
- RMSE (Root Mean Squared Error): Penalizes large errors
- R² (R-squared): Explains variance (0-1, higher is better)
- MAPE (Mean Absolute Percentage Error): For relative errors

Classification:
- Accuracy: Overall correctness (use when classes balanced)
- Precision: How many predicted positives are correct
- Recall: How many actual positives were found
- F1-Score: Harmonic mean of precision and recall
- ROC-AUC: Overall discrimination ability
- Confusion Matrix: Detailed error analysis

✓ DO: Use multiple metrics
✓ DO: Choose metrics that align with business goals
❌ DON'T: Rely on accuracy alone for imbalanced data


REPRODUCIBILITY REQUIREMENTS:
-----------------------------
1. Set random seeds everywhere:
   - Python: random.seed(42)
   - NumPy: np.random.seed(42)
   - Model-specific seeds

2. Document dependencies:
   - Create requirements.txt or environment.yml
   - Pin versions: scikit-learn==1.3.0 (not >=1.3.0)

3. Version control:
   - Git commit all code
   - Tag model versions with git tags
   - Include git hash in MLflow tags

4. Data versioning:
   - Use DVC (Data Version Control)
   - Or save data checksums
   - Document data sources and collection dates

================================================================================
5. MODEL DEPLOYMENT STRATEGIES
================================================================================

DEPLOYMENT PATTERNS:
-------------------

Pattern 1: REST API (Most Common)
Technology: FastAPI, Flask, Django REST
Use Case: Real-time predictions, low-to-medium volume
Pros: Easy to implement, language-agnostic clients
Cons: Synchronous, may not scale to very high volume

Pattern 2: Batch Prediction
Technology: Airflow, Prefect, scheduled scripts
Use Case: Periodic predictions, large datasets
Pros: Efficient for large volumes, can optimize resources
Cons: Not real-time, results delayed

Pattern 3: Streaming
Technology: Kafka, Kinesis, Spark Streaming
Use Case: Real-time, high-volume predictions
Pros: Handles high throughput, low latency
Cons: Complex infrastructure

Pattern 4: Embedded
Technology: TensorFlow Lite, ONNX
Use Case: Mobile/edge devices
Pros: No network required, fast inference
Cons: Limited model complexity, deployment challenges


API DESIGN BEST PRACTICES:
--------------------------

✓ DO: Use Pydantic for request/response validation

from pydantic import BaseModel, Field

class PredictionRequest(BaseModel):
    feature1: float = Field(..., ge=0, description="Must be positive")
    feature2: str = Field(..., regex="^(cat1|cat2|cat3)$")

class PredictionResponse(BaseModel):
    prediction: float
    confidence: float
    model_version: str


✓ DO: Include health check endpoint

@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "version": "1.0.0"
    }


✓ DO: Add versioning to API

@app.post("/v1/predict")  # Version in URL
def predict_v1(request: PredictionRequest):
    pass


✓ DO: Return prediction confidence/uncertainty

return {
    "prediction": prediction,
    "confidence_interval": [lower_bound, upper_bound],
    "probability": probability
}


✓ DO: Handle errors gracefully

from fastapi import HTTPException

try:
    prediction = model.predict(features)
except ValueError as e:
    raise HTTPException(status_code=400, detail=str(e))
except Exception as e:
    raise HTTPException(status_code=500, detail="Internal server error")


MODEL LOADING STRATEGIES:
-------------------------

Strategy 1: Load on Startup (Recommended for small models)
- Load model when application starts
- Fast inference, but increases startup time
- Suitable for models < 1GB

Strategy 2: Lazy Loading
- Load model on first prediction request
- Fast startup, but first request is slow
- Use caching to keep model in memory

Strategy 3: Model Server
- Dedicated model serving infrastructure
- Examples: TensorFlow Serving, TorchServe, MLflow Models
- Use for production-grade deployments


CRITICAL: Feature Transformation in Inference
---------------------------------------------

✓ CORRECT Pattern:

# Load BOTH model and preprocessor
model = joblib.load('model.pkl')
preprocessor = joblib.load('preprocessor.pkl')

def predict(input_data):
    # 1. Recreate any derived features
    input_data['feature_derived'] = input_data['a'] / input_data['b']

    # 2. Apply SAVED preprocessor
    features_transformed = preprocessor.transform(input_data)

    # 3. Predict
    prediction = model.predict(features_transformed)
    return prediction


❌ WRONG: Creating new transformations

def predict(input_data):
    # WRONG - creating new scaler instead of using saved one
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(input_data)
    prediction = model.predict(features_scaled)
    return prediction

================================================================================
6. CI/CD FOR ML PROJECTS
================================================================================

ML PIPELINE AUTOMATION STAGES:
------------------------------

Stage 1: Code Quality Checks
- Linting (flake8, pylint, black)
- Type checking (mypy)
- Security scanning (bandit)

Stage 2: Unit Testing
- Test data processing functions
- Test feature engineering logic
- Test model training functions
- Test API endpoints

Stage 3: Data Validation
- Check data schema
- Validate data quality
- Check for data drift

Stage 4: ML Pipeline Execution
- Run data processing
- Run feature engineering
- Run model training (optional - can be manual)

Stage 5: Model Validation
- Evaluate model performance
- Check metrics against thresholds
- Compare with baseline model

Stage 6: Deployment
- Build Docker images
- Deploy to staging
- Run integration tests
- Deploy to production (manual approval)


GITHUB ACTIONS PATTERN FOR ML:
------------------------------

Triggers:
✓ DO: Trigger on code changes (src/, configs/)
✓ DO: Trigger on data changes (data/raw/)
❌ DON'T: Trigger on every commit to any file

Artifact Management:
✓ DO: Save pipeline outputs as artifacts
✓ DO: Use appropriate retention periods (7-30 days)
✓ DO: Tag artifacts with version/commit hash

Job Dependencies:
✓ DO: Use needs: to create pipeline dependencies
✓ DO: Fail fast if upstream jobs fail

Example structure:
```yaml
jobs:
  data-processing:
    runs-on: ubuntu-latest
    steps:
      - name: Process data
      - name: Upload artifact

  feature-engineering:
    needs: data-processing
    runs-on: ubuntu-latest
    steps:
      - name: Download data artifact
      - name: Engineer features
      - name: Upload artifacts

  model-training:
    needs: feature-engineering
    runs-on: ubuntu-latest
    steps:
      - name: Download artifacts
      - name: Train model
      - name: Upload model
```


MANUAL GATES FOR MODEL DEPLOYMENT:
----------------------------------
✓ DO: Require manual approval for production deployment
✓ DO: Automate deployment to staging
✓ DO: Run automated tests in staging before production
❌ DON'T: Auto-deploy models to production without review


MODEL DEPLOYMENT CHECKLIST:
---------------------------
□ Model performance meets thresholds
□ No data leakage detected
□ Inference latency acceptable
□ API tests pass
□ Load testing completed
□ Rollback plan in place
□ Monitoring configured
□ Alerts set up

================================================================================
7. COMMON PITFALLS AND HOW TO AVOID THEM
================================================================================

PITFALL 1: Data Leakage
-----------------------
PROBLEM: Information from test set leaks into training

Common Causes:
❌ Fitting transformers on entire dataset
❌ Using global statistics (mean, std) from all data
❌ Feature engineering using future information

Prevention:
✓ Always fit on training data only
✓ Use pipelines to prevent leakage
✓ Be careful with time series features


PITFALL 2: Train-Serve Skew
---------------------------
PROBLEM: Different behavior in training vs production

Common Causes:
❌ Different feature engineering logic
❌ Different library versions
❌ Different data preprocessing
❌ Missing features in production

Prevention:
✓ Use same preprocessor in train and serve
✓ Pin all dependency versions
✓ Validate input schema
✓ Test inference pipeline with training data


PITFALL 3: Missing Experiment Tracking
--------------------------------------
PROBLEM: Can't reproduce results or compare models

Common Causes:
❌ Not logging hyperparameters
❌ Not saving random seeds
❌ Not tracking code versions

Prevention:
✓ Always use experiment tracking (MLflow)
✓ Log everything: params, metrics, artifacts
✓ Include git commit hash


PITFALL 4: Hardcoded Values
---------------------------
PROBLEM: Can't easily change configurations

Common Causes:
❌ File paths in code
❌ Hyperparameters in code
❌ Feature names in code

Prevention:
✓ Use configuration files
✓ Use command-line arguments
✓ Use environment variables


PITFALL 5: Not Handling Missing Models
--------------------------------------
PROBLEM: Experiment creation fails if model doesn't exist

Example Issue:
```python
# This fails if experiment doesn't exist
mlflow.set_tracking_uri(uri)
if uri:
    mlflow.set_experiment(name)  # Only sets if uri exists!

# Later...
mlflow.start_run()  # ERROR: No experiment set!
```

Solution:
```python
# Set tracking URI (optional)
if uri:
    mlflow.set_tracking_uri(uri)

# ALWAYS set experiment (creates if doesn't exist)
experiment = mlflow.set_experiment(name)
```


PITFALL 6: Version Mismatches
-----------------------------
PROBLEM: Model fails to load due to library version differences

Common Causes:
❌ Using unpinned dependencies (>=)
❌ Different Python versions
❌ Different OS/architecture

Prevention:
✓ Pin all versions: scikit-learn==1.3.0
✓ Use Docker for consistent environments
✓ Log Python and library versions with model
✓ Test model loading in clean environment


PITFALL 7: No Error Handling
----------------------------
PROBLEM: Pipeline fails without clear error messages

Prevention:
✓ Add try-except blocks with meaningful messages
✓ Log errors with context
✓ Validate inputs early
✓ Return informative error responses from APIs

================================================================================
8. DEBUGGING AND TROUBLESHOOTING
================================================================================

SYSTEMATIC DEBUGGING APPROACH:
------------------------------

Step 1: Isolate the Problem
- Which pipeline stage is failing?
- Is it data, features, model, or deployment?

Step 2: Check Inputs
- Verify input files exist
- Check file sizes and row counts
- Validate data schema

Step 3: Check Outputs
- Verify output files are created
- Check file sizes make sense
- Inspect first few rows

Step 4: Check Logs
- Read error messages carefully
- Look for stack traces
- Check MLflow logs

Step 5: Test in Isolation
- Run failing component separately
- Use minimal test data
- Add verbose logging


COMMON ERROR PATTERNS:
---------------------

Error: "Module not found"
Solution:
- Activate virtual environment
- Install dependencies: pip install -r requirements.txt
- Check Python path: which python

Error: "File not found"
Solution:
- Check file exists: ls -lh <file>
- Verify absolute vs relative paths
- Check file permissions

Error: "Could not find experiment with ID X"
Solution:
- Set experiment explicitly: mlflow.set_experiment()
- Check MLflow tracking URI is correct
- Verify MLflow server is running

Error: "Feature names don't match"
Solution:
- Ensure same features in train and inference
- Check feature order
- Verify preprocessor was saved correctly

Error: "Shape mismatch"
Solution:
- Check input dimensions
- Verify preprocessing is consistent
- Inspect feature counts


DEBUGGING TOOLS:
---------------

1. Logging:
```python
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info(f"Processing {len(df)} rows")
logger.debug(f"Features: {df.columns.tolist()}")
```

2. Assertions:
```python
assert df.shape[0] > 0, "Empty dataframe!"
assert 'price' in df.columns, "Missing target column"
```

3. Verification Scripts:
```python
# verify_pipeline.py
def verify_data_processing():
    assert os.path.exists('cleaned_data.csv')
    df = pd.read_csv('cleaned_data.csv')
    assert df.isnull().sum().sum() == 0
    print("✓ Data processing OK")

def verify_feature_engineering():
    assert os.path.exists('features.csv')
    assert os.path.exists('preprocessor.pkl')
    print("✓ Feature engineering OK")
```

================================================================================
9. PRODUCTION READINESS CHECKLIST
================================================================================

CODE QUALITY:
------------
□ All code is version controlled (Git)
□ Code follows style guide (PEP 8 for Python)
□ Code is well-documented
□ Unit tests written and passing
□ Integration tests written and passing
□ No hardcoded secrets or credentials

DATA:
----
□ Data validation checks in place
□ Data quality monitoring configured
□ Data versioning implemented
□ Backup strategy defined

MODEL:
-----
□ Model performance meets requirements
□ Model is registered in model registry
□ Model artifacts are versioned
□ Model can be loaded and used for inference
□ Preprocessing pipeline is saved and tested

EXPERIMENT TRACKING:
-------------------
□ All experiments logged to MLflow/Weights & Biases
□ Hyperparameters documented
□ Metrics tracked
□ Model artifacts saved
□ System information logged

DEPLOYMENT:
----------
□ API endpoints tested
□ Health check endpoint implemented
□ Error handling in place
□ Input validation implemented
□ Response time acceptable (<100ms for most use cases)
□ Load testing completed
□ Docker images built and tested

MONITORING:
----------
□ Application logs configured
□ Model performance monitoring set up
□ Data drift detection implemented
□ Alerts configured for failures
□ Metrics dashboards created

DOCUMENTATION:
-------------
□ README with setup instructions
□ API documentation generated
□ Model documentation written
□ Deployment guide created
□ Troubleshooting guide available

OPERATIONS:
----------
□ Rollback procedure documented
□ Incident response plan in place
□ On-call rotation defined
□ Runbook created

COMPLIANCE & SECURITY:
---------------------
□ Model bias evaluated
□ Privacy requirements met
□ Security scan completed
□ Access controls implemented
□ Audit logging enabled

================================================================================
KEY TAKEAWAYS
================================================================================

1. MODULARITY: Keep pipeline stages separate and testable

2. CONSISTENCY: Use same preprocessing in training and inference

3. TRACKING: Log everything - you never know what you'll need later

4. VERSIONING: Version code, data, models, and dependencies

5. TESTING: Test each component independently before integration

6. CONFIGURATION: Use config files, not hardcoded values

7. ERROR HANDLING: Fail gracefully with informative messages

8. DOCUMENTATION: Document assumptions, decisions, and procedures

9. MONITORING: Watch for data drift and performance degradation

10. AUTOMATION: Automate repetitive tasks, but keep humans in the loop
    for critical decisions

================================================================================
RESOURCES AND FURTHER READING
================================================================================

Books:
- "Designing Machine Learning Systems" by Chip Huyen
- "Machine Learning Engineering" by Andriy Burkov
- "Building Machine Learning Powered Applications" by Emmanuel Ameisen

Tools:
- Experiment Tracking: MLflow, Weights & Biases, Neptune.ai
- Feature Stores: Feast, Tecton
- Model Serving: BentoML, Seldon Core, KServe
- Orchestration: Airflow, Prefect, Kubeflow
- Monitoring: Evidently AI, WhyLabs, Arize

================================================================================
END OF GUIDE
================================================================================
