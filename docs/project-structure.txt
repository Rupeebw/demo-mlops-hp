================================================================================
HOUSE PRICE PREDICTOR - MLOps PROJECT STRUCTURE
================================================================================

demo-mlops-project/
│
├── README.md                          # Project overview and setup instructions
├── LICENSE                            # Project license information
├── CLAUDE.md                          # Claude Code AI assistant instructions
├── ChangeLog.txt                      # Project changelog and deployment history
├── requirements.txt                   # Python dependencies for the project
├── .gitignore                         # Git ignore patterns
├── docker-compose.yaml                # Multi-service deployment (API + UI)
├── Dockerfile                         # Container for FastAPI service
├── setup_env.sh                       # Environment setup script
├── libomp.dylib                       # OpenMP library symlink (macOS)
│
│
├── .github/                           # GitHub configuration
│   └── workflows/                     # GitHub Actions workflows
│       ├── ml-pipeline.yml            # Automated ML pipeline (CI/CD)
│       └── README.md                  # Workflow documentation
│
├── configs/                           # Configuration files directory
│   └── model_config.yaml              # Model hyperparameters and settings
│
├── data/                              # Data storage directory
│   ├── raw/                           # Original, immutable data
│   │   └── house_data.csv             # Source dataset with house features
│   └── processed/                     # Cleaned and transformed data
│       ├── README.md                  # Processing documentation
│       ├── cleaned_house_data.csv     # After data cleaning pipeline
│       └── featured_house_data.csv    # After feature engineering pipeline
│
├── docs/                              # Project documentation
│   ├── project-structure.txt          # This file - complete directory tree
│   ├── Dos-and-Dont-Rules.txt         # Project rules and guidelines
│   └── mlops-best-practices.txt       # Comprehensive MLOps best practices
│
├── notebooks/                         # Jupyter notebooks for exploration
│   ├── 00_data_engineering.ipynb      # Data cleaning and validation
│   ├── 01_exploratory_data_analysis.ipynb  # EDA and insights
│   ├── 02_feature_engineering.ipynb   # Feature creation experiments
│   └── 03_experimentation.ipynb       # Model training experiments
│
├── src/                               # Source code directory
│   ├── data/                          # Data processing scripts
│   │   └── run_processing.py          # Cleans raw data, handles outliers
│   │
│   ├── features/                      # Feature engineering scripts
│   │   └── engineer.py                # Creates features, saves preprocessor
│   │
│   ├── models/                        # Model training scripts
│   │   └── train_model.py             # Trains models, logs to MLflow
│   │
│   └── api/                           # FastAPI inference service
│       ├── main.py                    # FastAPI endpoints (predict, health)
│       ├── inference.py               # Loads model, performs predictions
│       ├── schemas.py                 # Pydantic request/response models
│       ├── utils.py                   # Helper functions
│       ├── requirements.txt           # API-specific dependencies
│       └── README.md                  # API documentation
│
├── models/                            # Trained model artifacts
│   └── trained/                       # Production-ready models
│       ├── README.md                  # Model versioning info
│       ├── house_price_model.pkl      # Trained regression model (XGBoost)
│       └── preprocessor.pkl           # Fitted sklearn preprocessor
│
├── streamlit_app/                     # Streamlit UI application
│   ├── app.py                         # Main Streamlit interface
│   ├── requirements.txt               # Streamlit dependencies
│   ├── README.md                      # UI documentation
│   ├── Dockerfile                     # Container for Streamlit app
│   └── .streamlit/                    # Streamlit configuration
│       └── config.toml                # UI theme and settings
│
└── deployment/                        # Deployment configurations
    ├── mlflow/                        # MLflow tracking server
    │   └── docker-compose.yaml        # MLflow server containerization
    │
    └── kubernetes/                    # K8s deployment manifests
        └── README.md                  # K8s deployment guide


================================================================================
DATA FLOW PIPELINE
================================================================================

1. DATA PROCESSING (src/data/run_processing.py)
   Input:  data/raw/house_data.csv
   Output: data/processed/cleaned_house_data.csv
   Tasks:  Handle missing values, remove outliers, validate data

2. FEATURE ENGINEERING (src/features/engineer.py)
   Input:  data/processed/cleaned_house_data.csv
   Output: data/processed/featured_house_data.csv
           models/trained/preprocessor.pkl
   Tasks:  Create derived features, fit preprocessor, transform data

3. MODEL TRAINING (src/models/train_model.py)
   Input:  data/processed/featured_house_data.csv
           configs/model_config.yaml
   Output: models/trained/house_price_model.pkl
   Tasks:  Train models, log to MLflow, register best model

4. INFERENCE (src/api/inference.py)
   Input:  models/trained/house_price_model.pkl
           models/trained/preprocessor.pkl
   Output: Price predictions with confidence intervals
   Tasks:  Load models, recreate features, transform input, predict


================================================================================
GITHUB ACTIONS CI/CD PIPELINE
================================================================================

Workflow File: .github/workflows/ml-pipeline.yml

Pipeline Stages:

1. DATA PROCESSING JOB
   - Checkout code
   - Setup Python 3.11 with UV
   - Install dependencies
   - Run data processing script
   - Upload cleaned data as artifact

2. FEATURE ENGINEERING JOB
   - Download cleaned data artifact
   - Run feature engineering
   - Create and save preprocessor
   - Upload featured data and preprocessor as artifacts

3. MODEL TRAINING JOB
   - Download featured data and preprocessor
   - Setup MLflow Docker server (port 5000)
   - Wait for MLflow health check
   - Train model with MLflow tracking
   - Upload trained model and MLflow artifacts
   - Cleanup MLflow container

4. BUILD AND PUBLISH DOCKER IMAGE JOB (Conditional)
   - Runs only on: main branch push OR version tags (v*.*.*)
   - Download model and preprocessor artifacts
   - Setup Docker Buildx
   - Login to DockerHub
   - Build and push Docker image with tags:
     * latest (for main branch)
     * Semantic versioning (v1.0.0, 1.0)
     * SHA-based tags (main-sha256:...)
   - Uses build cache for optimization

5. PIPELINE SUMMARY JOB
   - Aggregates all job statuses
   - Displays pipeline success/failure
   - Lists generated artifacts
   - Notes Docker build status (success/skipped)

Triggers:
- Push to main or develop branches
- Pull requests to main or develop
- Version tags (v*.*.*)
- Changes to: src/, data/raw/, configs/, .github/workflows/

Environment Requirements:
- DOCKERHUB_USERNAME (variable)
- DOCKERHUB_TOKEN (secret)


================================================================================
KEY FEATURES
================================================================================

ENGINEERED FEATURES:
- house_age         = current_year - year_built
- price_per_sqft    = price / sqft
- bed_bath_ratio    = bedrooms / bathrooms

NUMERICAL FEATURES:
- sqft, bedrooms, bathrooms, house_age, price_per_sqft, bed_bath_ratio

CATEGORICAL FEATURES:
- location (one-hot encoded)
- condition (one-hot encoded)


================================================================================
DEPLOYMENT ARCHITECTURE
================================================================================

┌──────────────────────────────────────────────────────────────┐
│  MLflow Tracking Server (localhost:5555)                     │
│  - Tracks experiments, parameters, metrics                   │
│  - Stores model artifacts                                    │
│  - Model registry (Staging/Production)                       │
│  - Docker deployment via deployment/mlflow/                  │
└──────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────┐
│  FastAPI Service (localhost:8000)                            │
│  Endpoints:                                                  │
│  - GET  /health        : Health check                        │
│  - POST /predict       : Single prediction                   │
│  - POST /batch-predict : Batch predictions                   │
│  - Dockerized via Dockerfile                                 │
└──────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────┐
│  Streamlit UI (localhost:8501)                               │
│  - User-friendly web interface                               │
│  - Input house features via forms                            │
│  - Display predictions with confidence intervals             │
│  - Connects to FastAPI backend                               │
│  - Dockerized via streamlit_app/Dockerfile                   │
└──────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────┐
│  GitHub Actions CI/CD                                        │
│  - Automated ML pipeline execution                           │
│  - MLflow Docker integration                                 │
│  - Docker image build and publish to DockerHub              │
│  - Artifact management and versioning                        │
└──────────────────────────────────────────────────────────────┘


================================================================================
TECHNOLOGY STACK
================================================================================

ML/DATA:        pandas, numpy, scikit-learn, xgboost
TRACKING:       MLflow (with Docker deployment)
API:            FastAPI, Pydantic, uvicorn
UI:             Streamlit
CONTAINERIZATION: Docker, Docker Compose
CI/CD:          GitHub Actions
ORCHESTRATION:  Kubernetes (optional)
TESTING:        pytest
NOTEBOOK:       Jupyter Lab
REGISTRY:       DockerHub (container images)
VERSION CONTROL: Git, GitHub





================================================================================
QUICK START COMMANDS
================================================================================

# Setup Environment
uv venv --python python3.11
source .venv/bin/activate
uv pip install -r requirements.txt

# Start MLflow Server
cd deployment/mlflow
docker compose -f mlflow-docker-compose.yml up -d
# Access at: http://localhost:5555

# Run ML Pipeline (Local)
python src/data/run_processing.py \
  --input data/raw/house_data.csv \
  --output data/processed/cleaned_house_data.csv

python src/features/engineer.py \
  --input data/processed/cleaned_house_data.csv \
  --output data/processed/featured_house_data.csv \
  --preprocessor models/trained/preprocessor.pkl

python src/models/train_model.py \
  --config configs/model_config.yaml \
  --data data/processed/featured_house_data.csv \
  --models-dir models \
  --mlflow-tracking-uri http://localhost:5555

# Run Tests
pytest
pytest -v  # verbose output

# Deploy Services (Docker Compose)
docker compose up -d
# or for Podman users:
podman compose up -d

# Access Services
MLflow UI:    http://localhost:5555
FastAPI:      http://localhost:8000
API Docs:     http://localhost:8000/docs
Streamlit UI: http://localhost:8501

# GitHub Actions (Automated Pipeline)
# Push to main branch or create version tag
git tag v1.0.0
git push origin v1.0.0
# Pipeline runs automatically, builds Docker image


================================================================================
BEST PRACTICES
================================================================================

1. Always run pipelines in order: processing → engineering → training
2. Retrain preprocessor when modifying feature engineering
3. Use MLflow for experiment tracking and model versioning
4. Test locally before containerization
5. Keep feature engineering consistent between training and inference
6. Version control model artifacts via MLflow Model Registry
7. Use configuration files for model hyperparameters
8. Document data transformations and feature definitions
9. Use GitHub Actions for automated pipeline execution
10. Secure DockerHub credentials in GitHub Secrets
11. Tag releases with semantic versioning (v1.0.0)
12. Review docs/ directory before code generation
13. Follow project guidelines in Dos-and-Dont-Rules.txt
14. Refer to mlops-best-practices.txt for universal patterns
15. Use custom Claude commands for common tasks


================================================================================
ARTIFACT MANAGEMENT
================================================================================

Local Artifacts:
- data/processed/*.csv          (pipeline outputs)
- models/trained/*.pkl          (trained models and preprocessors)
- mlruns/                       (MLflow tracking - gitignored)

GitHub Actions Artifacts:
- cleaned-data                  (data processing output)
- featured-data                 (feature engineering output)
- preprocessor                  (fitted preprocessor)
- trained-model                 (trained model pickle)
- mlflow-artifacts              (MLflow experiment data)
- Retention: 30 days

DockerHub Artifacts:
- <username>/house-price-model:latest
- <username>/house-price-model:v1.0.0
- <username>/house-price-model:1.0
- <username>/house-price-model:main-sha256:...


================================================================================
ENVIRONMENT VARIABLES
================================================================================

Required for GitHub Actions:
- DOCKERHUB_USERNAME (variable) : DockerHub account username
- DOCKERHUB_TOKEN (secret)      : DockerHub access token

Optional for Local Development:
- MLFLOW_TRACKING_URI           : MLflow server URL (default: http://localhost:5555)
- API_URL                       : FastAPI URL for Streamlit (default: http://fastapi:8000)


